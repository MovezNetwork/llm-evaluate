{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9636ff43-b756-45cd-9c0e-884c5434a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams, trigrams, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from textstat.textstat import textstatistics \n",
    "from collections import Counter\n",
    "from chat_analysis import *\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import configparser\n",
    "import numpy as np\n",
    "import inspect\n",
    "import sys\n",
    "# access parent directory from notebooks directory\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a1ae7-5c5d-461f-aa5e-a7eb38b56ea7",
   "metadata": {},
   "source": [
    "### Notebook for preprocessing of participants chat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba085a39-17ee-4327-9327-3d1148cc4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "url = config.get('credentials', 'surfdrive_url_movez_chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a26a3-fe4e-4f3d-b35f-0400f102c8d2",
   "metadata": {},
   "source": [
    "#### This code preprocesses the whatsapp files, creates a separate csv per participant, with a datetime, username, message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc75423-6bde-4079-bf8a-fb2f4fcb76c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:63: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:63: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:63: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:63: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:63: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:95: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_summary = pd.concat([df_summary, pd.DataFrame([{'username': username, 'word_count_median': df['word_count'].median(),\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['username'] = df['username'].map(df_users.set_index('username')['index'])\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_60118/748117351.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['username'] = df['username'].map(df_users.set_index('username')['index'])\n"
     ]
    }
   ],
   "source": [
    "output_chat_data_folder_path = 'output_chat_data/'\n",
    "# Regular expression pattern for parsing each txt file line\n",
    "pattern_rest = re.compile(rb'\\[((\\d{2}/\\d{2}/\\d{4})|(\\d{2}\\.\\d{2}\\.\\d{2})), (\\d{2}:\\d{2}:\\d{2})\\] (.*?): (.*)\\r\\n')\n",
    "pattern_te = re.compile(rb'(\\d{1,2}/\\d{1,2}/\\d{2,4},? \\d{2}:\\d{2}|\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}) - (.+?): (.*)\\n')\n",
    "# Send an HTTP GET request to the URL (\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}) \n",
    "response = requests.get(url)\n",
    "# dictionary with all the individuals dataframes\n",
    "df_dict = {}\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Create a BytesIO object to hold the downloaded ZIP file content\n",
    "    zip_content = BytesIO(response.content)\n",
    "    csv_file_to_read = 'movez_chats/movez_participants.csv'\n",
    "    # Use the zipfile module to extract the contents\n",
    "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
    "        zip_ref.extract(csv_file_to_read)\n",
    "        #extract the participants anonymization file\n",
    "        df_users = pd.read_csv(csv_file_to_read)\n",
    "\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Lists to store the extracted data from file\n",
    "            datetimes = []\n",
    "            usernames = []\n",
    "            messages = []\n",
    "            emojis = []\n",
    "            # Check if the file has a .txt extension\n",
    "            if file_info.filename.endswith('.txt'):\n",
    "                # Extract the content of the text file\n",
    "                with zip_ref.open(file_info.filename) as txt_file:\n",
    "                    # Read and print the content of the text file line by line\n",
    "                    if file_info.filename.endswith('Thabo.txt') or file_info.filename.endswith('Ying.txt'):\n",
    "                        for line in txt_file:\n",
    "                                match = pattern_te.match(line)\n",
    "                                if match:\n",
    "                                    group = match.groups()\n",
    "                                    datetimes.append(group[0].decode('utf-8'))\n",
    "                                    usernames.append(group[1].decode('utf-8'))\n",
    "                                    messages.append(group[2].decode('utf-8'))\n",
    "                                    emojis.append('')\n",
    "                    else:\n",
    "                        for line in txt_file:\n",
    "                            match = pattern_rest.match(line)\n",
    "                            if match:\n",
    "                                group = match.groups()\n",
    "                                # Combine date and time into a single string\n",
    "                                datetime_str = group[0].decode('utf-8') + ' ' + group[3].decode('utf-8')\n",
    "                                datetimes.append(datetime_str)\n",
    "                                usernames.append(group[4].decode('utf-8'))\n",
    "                                messages.append(group[5].decode('utf-8'))\n",
    "                                emojis.append('')                            \n",
    "                            \n",
    "                \n",
    "                    # Creating a DataFrame\n",
    "                    df_chats = pd.DataFrame({\n",
    "                        'datetime': datetimes,\n",
    "                        'username': usernames,\n",
    "                        'message': messages,\n",
    "                        'emojis': emojis\n",
    "                    })\n",
    "                                \n",
    "                    df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
    "                    # Filter rows where datetime is greater than January 25, 2024 - start of the data donation experiment\n",
    "                    df_chats = df_chats[df_chats['datetime'] > '2024-01-25']\n",
    "                    # Applied lexical functions per message\n",
    "                    df_chats[['message', 'emojis']] = df_chats['message'].apply(extract_emojis).tolist()\n",
    "                    df_chats['word_count'] = df_chats['message'].apply(word_count)\n",
    "                    df_chats['punctuation_count'] = df_chats['message'].apply(punctuation_count)\n",
    "                    df_chats['readability_score'] = df_chats['message'].apply(readability_score)\n",
    "                    df_chats['lexical_density'] = df_chats['message'].apply(lexical_density)\n",
    "                    \n",
    "                    # Extracting unique usernames\n",
    "                    unique_usernames = df_chats['username'].unique()\n",
    "                    \n",
    "                    # Creating a dictionary to hold the DataFrames for each unique username\n",
    "                    df_dict.update({username: df_chats[df_chats['username'] == username] for username in unique_usernames})\n",
    "    \n",
    "    df_summary = pd.DataFrame(columns=['username','word_count_median','punctuation_count_avg','vocabulary_diversity','emoji_avg','lexical_density','readability_score'])\n",
    "    \n",
    "    #creating corpus level features here\n",
    "    for df in df_dict.values():\n",
    "        # Extracting the username from the first row of the DataFrame\n",
    "        df['username'] = df['username'].map(df_users.set_index('username')['index'])\n",
    "        username = df['username'].iloc[0]\n",
    "        # Sanitize the username to ensure it's safe for use as a file name\n",
    "        sanitized_username = \"\".join([c for c in username if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "        \n",
    "        # Lexical features applied on the whole corpus\n",
    "        all_messages = '. '.join(df['message'].astype(str))\n",
    "        # words = word_tokenize(all_messages)\n",
    "        # unique_words = set(words)\n",
    "        # ttr = len(unique_words) / len(words) if words else 0\n",
    "\n",
    "        df_summary = pd.concat([df_summary, pd.DataFrame([{'username': username, 'word_count_median': df['word_count'].median(),\n",
    "                                        'punctuation_count_avg': round(df['punctuation_count'].mean(),2), \n",
    "                                        'vocabulary_diversity': round(vocabulary_diversity(all_messages),2),\n",
    "                                        'emoji_avg': df['emojis'].count()/ df.shape[0], 'lexical_density' : round(df['lexical_density'].mean(),2),\n",
    "                                        'readability_score' : round(df['readability_score'].mean(),2) }])], ignore_index=True)\n",
    "\n",
    "        # saved to separate csv files\n",
    "        df_grams = get_top_ngrams(all_messages, n=2)\n",
    "        df_grams.to_csv(output_chat_data_folder_path + username + '_ngram.csv',index=False)\n",
    "        \n",
    "        df_pos_distribution = pos_distribution(all_messages)\n",
    "        df_pos_distribution.to_csv(output_chat_data_folder_path + username + '_pos_distribution.csv',index=False)\n",
    "\n",
    "        df_top_pos = get_top_words_by_pos(all_messages)\n",
    "        df_top_pos.to_csv(output_chat_data_folder_path + username + '_top_10_pos.csv',index=False)\n",
    "        \n",
    "        df_word_length_distribution = word_length_distribution(all_messages)\n",
    "        df_word_length_distribution.to_csv(output_chat_data_folder_path + username + '_word_distribution.csv',index=False)\n",
    "\n",
    "        \n",
    "        # Constructing the filename\n",
    "        filename = output_chat_data_folder_path + f'{sanitized_username}_chat_llm.csv'\n",
    "        filename_whole_corpus = output_chat_data_folder_path +  f'{sanitized_username}_all.txt'\n",
    "        f = open(filename_whole_corpus,'w')\n",
    "        f.write(all_messages) #Give your csv text here.\n",
    "        ## Python will convert \\n to os.linesep\n",
    "        f.close()\n",
    "        # Saving the DataFrame to a CSV file\n",
    "        df.to_csv(filename, index=False)\n",
    "\n",
    "        df_summary.to_csv(output_chat_data_folder_path+'0_chat_preprocess_summary.csv',index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-evaluate)",
   "language": "python",
   "name": "llm-evaluate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
