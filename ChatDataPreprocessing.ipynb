{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9636ff43-b756-45cd-9c0e-884c5434a6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/bojansimoski/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams, trigrams, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from textstat.textstat import textstatistics \n",
    "from collections import Counter\n",
    "from chat_analysis import *\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba085a39-17ee-4327-9327-3d1148cc4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "url = config.get('credentials', 'surfdrive_url')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a26a3-fe4e-4f3d-b35f-0400f102c8d2",
   "metadata": {},
   "source": [
    "#### This code preprocesses the whatsapp files, creates a separate csv per participant, with a datetime, username, message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7dc75423-6bde-4079-bf8a-fb2f4fcb76c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:227: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'type_token_ratio'] = ttr\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:255: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'vocabulary_diversity'] = vocabulary_diversity(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:256: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'average_sentence_length'] = average_sentence_length(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'stop_word_freq'] = stop_word_frequency(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'type_token_ratio'] = ttr\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:255: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'vocabulary_diversity'] = vocabulary_diversity(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:256: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'average_sentence_length'] = average_sentence_length(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'stop_word_freq'] = stop_word_frequency(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:227: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'type_token_ratio'] = ttr\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:255: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'vocabulary_diversity'] = vocabulary_diversity(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:256: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'average_sentence_length'] = average_sentence_length(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'stop_word_freq'] = stop_word_frequency(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'type_token_ratio'] = ttr\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:255: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'vocabulary_diversity'] = vocabulary_diversity(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:256: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'average_sentence_length'] = average_sentence_length(all_messages)\n",
      "/var/folders/t4/v59wrn6n2ws6kl1t9zh1q_mc0000gn/T/ipykernel_50702/577892426.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,'stop_word_freq'] = stop_word_frequency(all_messages)\n"
     ]
    }
   ],
   "source": [
    "output_chat_data_folder_path = 'output_chat_data/'\n",
    "\n",
    "def tokenize_messages(message):\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "# Word Count\n",
    "def word_count(message):\n",
    "    return len(tokenize_messages(message))\n",
    "\n",
    "# Use of Punctuation\n",
    "def punctuation_count(message):\n",
    "    return sum(1 for char in message if char in string.punctuation)\n",
    "\n",
    "# TTR is a measure of lexical richness that compares the number of unique words (types) to \n",
    "# the total number of words (tokens) in the text.\n",
    "# Unlike vocabulary diversity, which is calculated per message,\n",
    "# TTR can be calculated over varying lengths of text and is normalized for text length.\n",
    "# def type_token_ratio(message):\n",
    "#     words = nltk.word_tokenize(message)\n",
    "#     return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "# Compute readability indices like Flesch Reading Ease or Gunning Fog Index to assess the complexity of the text.\n",
    "# These scores can indicate how accessible or challenging the text is for readers.\n",
    "def readability_score(message):\n",
    "    return round(textstatistics().flesch_reading_ease(' '.join(message)), 2)\n",
    "\n",
    "# Lexical density is the proportion of content words (nouns, verbs, adjectives, adverbs) in the text\n",
    "# compared to the total number of words. A higher lexical density might indicate a more content-focused or formal style\n",
    "def lexical_density(message):\n",
    "    content_pos = {'NN', 'VB', 'JJ', 'RB'}  # Nouns, Verbs, Adjectives, Adverbs\n",
    "    words = nltk.word_tokenize(message)\n",
    "    tags = pos_tag(words)\n",
    "    content_words = sum(1 for word, tag in tags if tag in content_pos)\n",
    "    return content_words / len(words) if words else 0\n",
    "\n",
    "\n",
    "# Applied on the whole message corpus\n",
    "# Vocabulary Diversity: This function calculates the ratio of unique words to total words in the aggregated text.\n",
    "# It first tokenizes the text into words, converts them to lowercase for standardization,\n",
    "# and then calculates the ratio.\n",
    "def vocabulary_diversity(corpus):\n",
    "    words = word_tokenize(corpus)\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "    \n",
    "# Average Sentence Length: This function calculates the average number of words per sentence in\n",
    "# the aggregated text. It tokenizes the text into sentences and then counts the words in each sentence.\n",
    "def average_sentence_length(corpus):\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    return sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "\n",
    "# Analyze the frequency of bigrams (pairs of words) or trigrams (triplets of words). \n",
    "# This can reveal common phrases or topics in the chat data.\n",
    "# N-grams can be particularly insightful for identifying colloquial expressions or recurring themes.\n",
    "def ngram_frequency(corpus, n=2):\n",
    "    # Tokenize the corpus and generate n-grams\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    if n == 2:\n",
    "        ngrams = list(bigrams(words))\n",
    "    elif n == 3:\n",
    "        ngrams = list(trigrams(words))\n",
    "    else:\n",
    "        raise ValueError(\"n should be 2 for bigrams or 3 for trigrams\")\n",
    "\n",
    "    # Calculate frequency distribution\n",
    "    ngram_freq = Counter(ngrams)\n",
    "    return ngram_freq\n",
    "    \n",
    "# Function to calculate bigram frequency for each message\n",
    "def calculate_bigram_frequency(message):\n",
    "    return corpus_bigram_freq\n",
    "\n",
    "# Function to calculate trigram frequency for each message\n",
    "def calculate_trigram_frequency(message):\n",
    "    return corpus_trigram_freq\n",
    "    \n",
    "def pos_distribution(corpus):\n",
    "    # Tokenize the corpus and get POS tags\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Calculate frequency distribution of POS tags\n",
    "    pos_freq = FreqDist(tag for (word, tag) in pos_tags)\n",
    "    return pos_freq\n",
    "\n",
    "def word_length_distribution(corpus):\n",
    "    # Tokenize the corpus and get word lengths\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    word_lengths = [len(word) for word in words if word.isalpha()]\n",
    "\n",
    "    # Calculate frequency distribution of word lengths\n",
    "    length_freq = FreqDist(word_lengths)\n",
    "    return length_freq\n",
    "    \n",
    "# The term \"stop word frequency\" refers to the distribution of stop words in a text or corpu\n",
    "def stop_word_frequency(corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    total_words = len(words)\n",
    "    stop_words_count = sum(1 for word in words if word.lower() in stop_words)\n",
    "\n",
    "    return stop_words_count / total_words if total_words > 0 else 0\n",
    "    \n",
    "def get_top_ngrams(corpus, n=2, min_frequency=3):\n",
    "    # Tokenize the corpus into words\n",
    "    words = word_tokenize(corpus)\n",
    "    \n",
    "    # Exclude specific punctuation (e.g., \".\")\n",
    "    filtered_words = [word.lower() for word in words if word.isalpha() and word != \".\"]\n",
    "\n",
    "    # Generate n-grams\n",
    "    ngrams_list = list(bigrams(words)) if n == 2 else []\n",
    "\n",
    "    # Calculate frequency distribution of n-grams\n",
    "    ngram_freq = Counter(ngrams_list)\n",
    "\n",
    "    # Filter and print n-grams with frequency greater than min_frequency\n",
    "    top_ngrams = [(ngram, count) for ngram, count in ngram_freq.items() if count > min_frequency]\n",
    "\n",
    "    for ngram, count in top_ngrams:\n",
    "        print(f\"{n}-gram: {ngram}, Frequency: {count}\")\n",
    "\n",
    "def get_top_words_by_pos(corpus):\n",
    "    # Tokenize the corpus into words\n",
    "    words = word_tokenize(corpus)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    print(filtered_words)\n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(filtered_words)\n",
    "\n",
    "    # Separate words by POS category\n",
    "    verbs = [word for word, pos in pos_tags if pos.startswith('VB')]\n",
    "    adjectives = [word for word, pos in pos_tags if pos.startswith('JJ')]\n",
    "    nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "    pronouns = [word for word, pos in pos_tags if pos.startswith('PRP')]\n",
    "\n",
    "    # Count occurrences of each category\n",
    "    verb_counts = Counter(verbs)\n",
    "    adjective_counts = Counter(adjectives)\n",
    "    noun_counts = Counter(nouns)\n",
    "    pronoun_counts = Counter(pronouns)\n",
    "\n",
    "    # Get the top 10 used words for each category along with frequency\n",
    "    top_verbs = [(word, count) for word, count in verb_counts.most_common(10)]\n",
    "    top_adjectives = [(word, count) for word, count in adjective_counts.most_common(10)]\n",
    "    top_nouns = [(word, count) for word, count in noun_counts.most_common(10)]\n",
    "    top_pronouns = [(word, count) for word, count in pronoun_counts.most_common(10)]\n",
    "\n",
    "    return top_verbs, top_adjectives, top_nouns, top_pronouns\n",
    "        \n",
    "def extract_emojis(s):\n",
    "    # Extract emojis using the emoji package\n",
    "    all_emojis = ''.join(c for c in s if c in emoji.EMOJI_DATA)\n",
    "    \n",
    "    # Regex pattern for keyboard emoticons\n",
    "    emoticon_pattern = re.compile(r'(:\\)|;\\)|:\\(|:\\D|:P|:O|:\\||>:O|:\\/|:\\[|:\\]|:\\{|:\\}|<3)')\n",
    "\n",
    "    # Find all emoticons in the string\n",
    "    found_emoticons = emoticon_pattern.findall(s)\n",
    "    all_emoticons = ''.join(found_emoticons)\n",
    "\n",
    "    # Combine emojis and emoticons\n",
    "    all_emojis_emoticons = all_emojis + all_emoticons\n",
    "\n",
    "    # Remove emojis and emoticons from the original message\n",
    "    # Create a pattern that matches all found emojis and emoticons\n",
    "    combined_pattern = re.compile('|'.join(re.escape(c) for c in all_emojis) + '|' + emoticon_pattern.pattern)\n",
    "    \n",
    "    cleaned_message = combined_pattern.sub(r'', s)\n",
    "\n",
    "    # Return a tuple of cleaned message and emojis/emoticons\n",
    "    return cleaned_message, all_emojis_emoticons or 'N/A'  # Returns 'N/A' if none found\n",
    "\n",
    "\n",
    "# Regular expression pattern for parsing each txt file line\n",
    "pattern = re.compile(rb'\\[((\\d{2}/\\d{2}/\\d{4})|(\\d{2}\\.\\d{2}\\.\\d{2})), (\\d{2}:\\d{2}:\\d{2})\\] (.*?): (.*)\\r\\n')\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Create a BytesIO object to hold the downloaded ZIP file content\n",
    "    zip_content = BytesIO(response.content)\n",
    "\n",
    "    # Use the zipfile module to extract the contents\n",
    "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Lists to store the extracted data from file\n",
    "            datetimes = []\n",
    "            usernames = []\n",
    "            messages = []\n",
    "            emojis = []\n",
    "            # Check if the file has a .txt extension\n",
    "            if file_info.filename.endswith('.txt'):\n",
    "                # Extract the content of the text file\n",
    "                with zip_ref.open(file_info.filename) as txt_file:\n",
    "                    # Read and print the content of the text file line by line\n",
    "                    for line in txt_file:\n",
    "                        match = pattern.match(line)\n",
    "                        if match:\n",
    "                            group = match.groups()\n",
    "                            # Combine date and time into a single string\n",
    "                            datetime_str = group[0].decode('utf-8') + ' ' + group[3].decode('utf-8')\n",
    "                            datetimes.append(datetime_str)\n",
    "                            usernames.append(group[4].decode('utf-8'))\n",
    "                            messages.append(group[5].decode('utf-8'))\n",
    "                            emojis.append('')\n",
    "                            \n",
    "                \n",
    "                    # Creating a DataFrame\n",
    "                    df_chats = pd.DataFrame({\n",
    "                        'datetime': datetimes,\n",
    "                        'username': usernames,\n",
    "                        'message': messages,\n",
    "                        'emojis': emojis\n",
    "                    })\n",
    "                                \n",
    "                    df_chats['datetime'] = pd.to_datetime(df_chats['datetime'], errors='coerce')\n",
    "                    # Filter rows where datetime is greater than January 25, 2024 - start of the data donation experiment\n",
    "                    df_chats = df_chats[df_chats['datetime'] > '2024-01-25']\n",
    "                    # Applied lexical functions per message\n",
    "                    df_chats[['message', 'emojis']] = df_chats['message'].apply(extract_emojis).tolist()\n",
    "                    df_chats['word_count'] = df_chats['message'].apply(word_count)\n",
    "                    df_chats['punctuation_count'] = df_chats['message'].apply(punctuation_count)\n",
    "                    df_chats['readability_score'] = df_chats['message'].apply(readability_score)\n",
    "                    df_chats['lexical_density'] = df_chats['message'].apply(lexical_density)\n",
    "                    \n",
    "                    # Extracting unique usernames\n",
    "                    unique_usernames = df_chats['username'].unique()\n",
    "                    \n",
    "                    # Creating a dictionary to hold the DataFrames for each unique username\n",
    "                    df_dict = {username: df_chats[df_chats['username'] == username] for username in unique_usernames}\n",
    "                    \n",
    "                    for df in df_dict.values():\n",
    "                        # Extracting the username from the first row of the DataFrame\n",
    "                        username = df['username'].iloc[0]\n",
    "                        # Sanitize the username to ensure it's safe for use as a file name\n",
    "                        sanitized_username = \"\".join([c for c in username if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "                        \n",
    "                        # Lexical features applied on the whole corpus\n",
    "                        all_messages = '. '.join(df['message'].astype(str))\n",
    "                        words = word_tokenize(all_messages)\n",
    "                        unique_words = set(words)\n",
    "                        ttr = len(unique_words) / len(words) if words else 0\n",
    "                        df['type_token_ratio'] = ttr\n",
    "                        df['vocabulary_diversity'] = vocabulary_diversity(all_messages)\n",
    "                        df['average_sentence_length'] = average_sentence_length(all_messages)    \n",
    "                        df['stop_word_freq'] = stop_word_frequency(all_messages)\n",
    "                    \n",
    "                        # print('USERNAME: ', username,' \\n')\n",
    "                        # print('2-GRAM FREQUENCY \\n')\n",
    "                        # print(get_top_ngrams(all_messages, n=2))\n",
    "                        # print('\\nPOS DISTRIBUTION \\n')\n",
    "                        # fdist = pos_distribution(all_messages)\n",
    "                        # for f in fdist:\n",
    "                        #     print (f, fdist[f])\n",
    "                        # print('\\nPOS TOP 10 \\n')\n",
    "                        # print(get_top_words_by_pos(all_messages))\n",
    "                        # print('\\n WORD LENGTH DISTRIBUTION \\n')\n",
    "                        # word_length_d = word_length_distribution(all_messages)\n",
    "                        # for k in word_length_d:\n",
    "                        #     print (k, word_length_d[k])\n",
    "                \n",
    "                        \n",
    "                        # Constructing the filename\n",
    "                        filename = output_chat_data_folder_path + f'{sanitized_username}_chat_llm.csv'\n",
    "                    \n",
    "                        filename_whole_corpus = output_chat_data_folder_path +  f'{sanitized_username}_all.txt'\n",
    "                    \n",
    "                        f = open(filename_whole_corpus,'w')\n",
    "                        f.write(all_messages) #Give your csv text here.\n",
    "                        ## Python will convert \\n to os.linesep\n",
    "                        f.close()\n",
    "                        # Saving the DataFrame to a CSV file\n",
    "                        df.to_csv(filename, index=False)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6201101-c5df-496c-85ef-83de0ff44f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert DataFrame to CSV string\n",
    "# csv_content = df.to_csv(index=False)\n",
    "\n",
    "# # Set the target URL where you want to upload the CSV file\n",
    "# upload_url = 'https://surfdrive.surf.nl/files/index.php/s/yauByOwvpZmc5EE'\n",
    "\n",
    "# # Step 1: Download the existing ZIP folder\n",
    "# response = requests.get(upload_url)\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     # Create a BytesIO object to hold the downloaded ZIP file content\n",
    "#     zip_content = BytesIO(response.content)\n",
    "\n",
    "#     # Step 2: Add the new CSV file to the downloaded ZIP folder\n",
    "#     with zipfile.ZipFile(zip_content, 'a') as zip_file:\n",
    "#         zip_file.writestr('example.csv', csv_content)\n",
    "\n",
    "#     # Set the target URL for uploading the updated ZIP folder (same as the original)\n",
    "#     updated_upload_url = upload_url\n",
    "\n",
    "#     # Update the existing ZIP folder directly using PUT\n",
    "#     put_response = requests.put(updated_upload_url, data=zip_content.getvalue())\n",
    "\n",
    "#     # Check if the update was successful\n",
    "#     if put_response.status_code == 200:\n",
    "#         print(f\"ZIP folder successfully updated at {updated_upload_url}\")\n",
    "#     else:\n",
    "#         print(f\"Failed to update ZIP folder. Status code: {put_response.status_code}\")\n",
    "#         print(put_response.text)\n",
    "# else:\n",
    "#     print(f\"Failed to download existing ZIP folder. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700d556-ca67-473e-b562-47631f5f59f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b87cdd-16e4-4287-b7a4-870c1502e0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b4a05-83a8-4723-9be6-8e0494c7af20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-evaluate)",
   "language": "python",
   "name": "llm-evaluate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
