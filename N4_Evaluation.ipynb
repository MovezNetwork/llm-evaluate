{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc635030aa20c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:46:26.519854Z",
     "start_time": "2024-02-22T09:46:24.356586Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import s4_evaluation as eval\n",
    "\n",
    "# access parent directory from notebooks directory\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847533057e759c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:46:26.556155Z",
     "start_time": "2024-02-22T09:46:26.522770Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "api_key_openai = config.get('credentials', 'api_key_openai')\n",
    "api_key_mistral = config.get('credentials', 'api_key_mistral')\n",
    "surfdrive_url_transcript_sentences = config.get('credentials', 'surfdrive_url_transcript_sentences')\n",
    "\n",
    "output_processed_llm_tst_data = 'f7_processed_llm_tst_data/'\n",
    "output_evaluation_folder_path = 'output_evaluation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f17f76cbbd2ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42074043792409ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:46:26.880048Z",
     "start_time": "2024-02-22T09:46:26.557553Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2390\n",
      "2400\n"
     ]
    }
   ],
   "source": [
    "# read output of LLM\n",
    "fileNoContext = 'Prompts_NoContext_complete_output.csv'\n",
    "fileContext = 'Prompts_Context_complete_output.csv'\n",
    "\n",
    "outputNoContext = pd.read_csv(output_processed_llm_tst_data+fileNoContext)\n",
    "outputContext = pd.read_csv(output_processed_llm_tst_data+fileContext)\n",
    "# read rewritten sentences by participants\n",
    "transcriptSentences = pd.read_csv(surfdrive_url_transcript_sentences).reset_index()[['user', 'original', 'your_text']]\n",
    "\n",
    "# merge two files\n",
    "data_for_evaluation_noContext = pd.merge(outputNoContext, transcriptSentences, on=['user', 'original'])\n",
    "data_for_evaluation_context = pd.merge(outputContext, transcriptSentences, on=['user', 'original'])\n",
    "print(len(data_for_evaluation_noContext))\n",
    "print(len(data_for_evaluation_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d38b55e7d7961c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:46:26.923419Z",
     "start_time": "2024-02-22T09:46:26.877354Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "2349\n",
      "91\n",
      "2309\n"
     ]
    }
   ],
   "source": [
    "# check and remove empty files\n",
    "\n",
    "print(data_for_evaluation_noContext['rewritten_sentence'].isna().sum().sum())\n",
    "data_for_evaluation_noContext = data_for_evaluation_noContext.dropna(subset=['rewritten_sentence'])\n",
    "print(len(data_for_evaluation_noContext))\n",
    "\n",
    "print(data_for_evaluation_context['rewritten_sentence'].isna().sum().sum())\n",
    "data_for_evaluation_context = data_for_evaluation_context.dropna(subset=['rewritten_sentence'])\n",
    "print(len(data_for_evaluation_context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b977b534-81b8-457e-9502-86598169f8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fileID', 'user', 'promptID', 'model', 'shots', 'runID', 'original',\n",
       "       'rewritten_sentence', 'your_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_evaluation_noContext.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cd732f-9f50-490d-95ab-b7fcda8c6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shots_data = 'f4_shots_data/'\n",
    "username = 'U1'\n",
    "mistral_file = output_shots_data + username + '_mistral_shots_5.csv'\n",
    "df_m = pd.read_csv(mistral_file)\n",
    "user_style_string = '; '.join(df_m['original'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "676d60d7-9481-4dcb-9127-afbe36fe79d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ah I think its fine! this is probably related to their privacy statement; tell me about your family, cos we are running out of time; i ll do the export, so yeah we are done!; im curious, I ll start with some analysis later. thanks Kim!!; no skiing for me, I never learned it. I think im a bit scared to do that'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_style_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ae261bfee7dfb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Evaluation Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26e5ddbeca8ddf8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:46:26.966839Z",
     "start_time": "2024-02-22T09:46:26.917317Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# this should be read from surfdrive\n",
    "prompts_dict = {\n",
    "    # Generic prompt\n",
    "    'prompt_llm': 'You are an expert in text style transfer. Here is paragraph S1: {} ',\n",
    "                 \n",
    "    # Accuracy\n",
    "    'prompt_accuracy_s2': 'and paragraph S2: {} ',\n",
    "    'prompt_accuracy_inference': 'How different is the conversational style of paragraph S2 compared to S1 on a continuous scale from 1 (completely identical styles) to 10 (completely different styles)? Result = . Format result as \"score\" and \"explanation\".',\n",
    "    \n",
    "    # Content preservation\n",
    "    'prompt_content_preservation_s2': 'and paragraph S2: {} ',\n",
    "    'prompt_content_preservation_inference': 'How much does S1 preserve the content of S2 on a continuous scale from 1 (completely different topic) to 10 (identical topic)? Result = .Format result as \"score\" and \"explanation\".',\n",
    "    \n",
    "    # Fluency\n",
    "    'prompt_fluency_inference': 'on a scale from 1 to 10 where 1 (lowest coherent) and 10 (highest coherent)? Result = Format result as \"score\" and \"explanation\".'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "500ff624-e56a-4fb7-899f-9c10870dcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict_new_accuracy = {\n",
    "    # Generic prompt\n",
    "    'prompt_llm': 'You are an expert in text style transfer. Here is text T1, supposedly writen in a style of person X: {} ',\n",
    "                 \n",
    "    # Accuracy\n",
    "    'prompt_accuracy_s2': 'and here is another text T2, containing are a set of unrelated sentences (separated with ;), that are actually written by person X: {} ',\n",
    "    'prompt_accuracy_inference': 'How different is the conversational style in T1 and T2 on a continuous scale from 1 (completely identical styles) to 10 (completely different styles)? For scoring focus on semantics and syntax. Difference in discussed topics or contexts is irrelevant for the score. Result = . Format result as \"score\" and \"explanation\".',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86cac98275b1bd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Evaluation using Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7876cd19a0dd8e9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T09:46:42.547747Z",
     "start_time": "2024-02-22T09:46:29.007164Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# No Context \n",
    "# Run00\n",
    "# for index, row in data_for_evaluation_noContext.iterrows():\n",
    "#     print(index)\n",
    "#     name= 'NoContext_' + format(index)\n",
    "#     evaluation_noContext_mistral = eval.get_evaluation_mistral(prompts_dict, row, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "203b6714-8619-48a1-a0ca-31773967ce3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2349, 9)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_evaluation_noContext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b3926aa-d8d2-4bd6-b822-23e43d6592b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452\n",
      "prompt query:  You are an expert in text style transfer. \n",
      " Here is text T1: {So, I'm all about fast food, you know? Having some fries, a soy burger and coke at the burger place near school. And there's a football game this weekend!}  \n",
      " Here is another text T2:  {he looks more like a family guy; i think these will be very good matches, two teams fo similar strength; You remember Tonka put her name on a random mug?; We can do uber driver instead haha; do you have an update on study design?} \n",
      " \n",
      "    T1 is a coherent small paragraph, while T2 contains a set of unrelated sentences. (separated with ;). How different is the conversational style in T1 and T2 on a continuous scale from 1 (completely identical styles) to 10 (completely different styles)? \n",
      "    For scoring focus on semantics and syntax. Difference in discussed topics or contexts is irrelevant for the score. Result = . Format result as \"score\" and \"explanation\".  \n",
      "\n",
      "prompt response:  Score: 9\n",
      "\n",
      "Explanation:\n",
      "The conversational styles of T1 and T2 are very different in terms of semantics and syntax. T1 is a single, coherent paragraph that tells a story about someone's fast food preferences and their plans to attend a football game. The syntax is simple and straightforward, with short sentences and a clear subject-verb-object structure. The semantics are consistent, focusing on the theme of fast food and social activities.\n",
      "\n",
      "In contrast, T2 is a collection of unrelated sentences, each with its own syntax and semantics. The first sentence seems to describe someone's appearance or personality, while the second sentence discusses two sports teams. The third sentence refers to a mug with someone's name on it, and the fourth sentence suggests using an Uber driver for transportation. Finally, the last sentence asks for an update on a study design. The syntax is more varied in T2, with longer and more complex sentences, and the semantics are diverse and unrelated.\n",
      "\n",
      "Therefore, based on the differences in syntax and semantics, I would rate the conversational styles of T1 and T2 as a 9 on a scale of 1 to 10, where 1 represents completely identical styles and 10 represents completely different styles.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in data_for_evaluation_noContext[444:445].iterrows():\n",
    "    print(index)\n",
    "    name= 'NoContext_' + format(index)\n",
    "    evaluation_noContext_mistral = eval.get_updated_evaluation_mistral(prompts_dict_accuracy_evaluation, row, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc010f8e05547752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T03:08:37.764883Z",
     "start_time": "2024-02-21T03:08:37.761906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m all about that food ahahah. I usually kick off the week with some beans and vegan sausage just before school. And I’m off to school soon. I’m such a foodie. I usually start the week with some beans and vegan sausage before school. I’m off to school soon, gotta go\n"
     ]
    }
   ],
   "source": [
    "# Context \n",
    "# Run00\n",
    "for index, row in data_for_evaluation_context[:1].iterrows():\n",
    "    name= 'Context_' + format(index)\n",
    "    # evaluation_context_mistral = eval.get_evaluation_mistral(prompts_dict, row, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e677c1352de03b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Evaluation using GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f4b73c21e8d56",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T03:08:37.768105Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# No Context \n",
    "# Run00\n",
    "for index, row in data_for_evaluation_noContext.iterrows():\n",
    "    print(index)\n",
    "    name= 'NoContext_' + format(index)\n",
    "    evaluation_noContext_gpt = eval.get_evaluation_gpt(prompts_dict, row, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff52b224959716ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T10:12:15.809385Z",
     "start_time": "2024-02-22T09:47:04.533327Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2311\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2333\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n"
     ]
    }
   ],
   "source": [
    "# Context \n",
    "# Run00\n",
    "for index, row in data_for_evaluation_context[2200:2400].iterrows():\n",
    "    print(index)\n",
    "    name= 'Context_' + format(index)\n",
    "    evaluation_context_gpt = eval.get_evaluation_gpt(prompts_dict, row, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a4b7687e8dfc7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-evaluate)",
   "language": "python",
   "name": "llm-evaluate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
