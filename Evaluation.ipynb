{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import evaluation as eval\n",
    "\n",
    "# access parent directory from notebooks directory\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T21:10:51.263163Z",
     "start_time": "2024-02-19T21:10:51.037952Z"
    }
   },
   "id": "bcc635030aa20c5",
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "# Read the configuration file\n",
    "config.read('config.ini')\n",
    "api_key_openai = config.get('credentials', 'api_key_openai')\n",
    "api_key_mistral = config.get('credentials', 'api_key_mistral')\n",
    "surfdrive_url_transcript_sentences = config.get('credentials', 'surfdrive_url_transcript_sentences')\n",
    "\n",
    "output_evaluation_folder_path = 'output_evaluation/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:43:54.937149Z",
     "start_time": "2024-02-19T14:43:54.889768Z"
    }
   },
   "id": "847533057e759c23",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3f17f76cbbd2ed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read output of LLM\n",
    "fileWithoutContext = 'Prompts_NoContext_complete_output.csv'\n",
    "fileWithContext = 'Prompts_Context_complete_output.csv'\n",
    "\n",
    "outputWithoutContext = pd.read_csv(output_evaluation_folder_path+file)\n",
    "outputWithContext = pd.read_csv(output_evaluation_folder_path+file)\n",
    "\n",
    "# read rewritten sentences by participants\n",
    "transcriptSentences = pd.read_csv(surfdrive_url_transcript_sentences).reset_index()[['user', 'original', 'your_text']]\n",
    "\n",
    "# merge two files\n",
    "data_for_evaluation_withoutContext = pd.merge(outputWithoutContext, transcriptSentences, on=['user', 'original'])\n",
    "data_for_evaluation_withContext = pd.merge(outputWithContext, transcriptSentences, on=['user', 'original'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T14:54:56.236940Z",
     "start_time": "2024-02-19T14:54:55.946934Z"
    }
   },
   "id": "541645e7dc9353e3",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation Prompts\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af6ae261bfee7dfb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompts_dict = {\n",
    "    # Generic prompt\n",
    "    'prompt_llm': 'You are an expert in text style transfer. Here is paragraph S1: {} ',\n",
    "                 \n",
    "    # Accuracy\n",
    "    'prompt_accuracy_s2': 'and paragraph S2: {} ',\n",
    "    'prompt_accuracy_inference': 'How different is the conversational style of paragraph S2 compared to S1 on a continuous scale from 1 (completely identical styles) to 10 (completely different styles)? Result = (Only provide the score)',\n",
    "    \n",
    "    # Content preservation\n",
    "    'prompt_content_preservation_s2': 'and paragraph S2: {} ',\n",
    "    'prompt_content_preservation_inference': 'How much does S1 preserve the content of S2 on a continuous scale from 1 (completely different topic) to 10 (identical topic)? Result = (Only provide the score)',\n",
    "    \n",
    "    # Fluency\n",
    "    'prompt_fluency_inference': 'on a scale from 1 to 10 where 1 (lowest coherent) and 10 (highest coherent)? Result = (Only provide the score)'\n",
    "}\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T21:26:08.787728Z",
     "start_time": "2024-02-19T21:26:08.626951Z"
    }
   },
   "id": "a3cfa5de4a40be09",
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation using Mistral"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a86cac98275b1bd0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# without context\n",
    "output_withoutContext = eval.get_evaluation_mistral(prompts_dict, data_for_evaluation_withoutContext, 'withoutContext')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f199f682835a886"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# with context\n",
    "output_withContext = eval.get_evaluation_mistral(prompts_dict, data_for_evaluation_withoutContext, 'withContext')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac053e59971ff8f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation using GPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84e677c1352de03b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# without context\n",
    "output_withoutContext = eval.get_evaluation_gpt(prompts_dict, data_for_evaluation_withoutContext, 'withoutContext')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e1f4b73c21e8d56"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# with context\n",
    "output_withContext = eval.get_evaluation_gpt(prompts_dict, data_for_evaluation_withoutContext, 'withContext')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08e88890cf6a08"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### some testing (ignore)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aadbf5165a623005"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output_df = data_for_evaluation_withContext\n",
    "scores = []\n",
    "for index, row in output_df.iterrows():\n",
    "\n",
    "    prompt_accuracy = eval.get_accuracy_prompt(prompts_dict, row)\n",
    "    prompt_content_preservation = eval.get_content_preservation_prompt(prompts_dict, row)\n",
    "    prompt_fluency = eval.get_fluency_prompt(prompts_dict, row)\n",
    "\n",
    "    score = 8\n",
    "    scores.append(score)\n",
    "\n",
    "    if row[0] == 0:\n",
    "        print(prompt_accuracy)\n",
    "\n",
    "output_df['score'] = scores\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62d2a4d92191a4dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
