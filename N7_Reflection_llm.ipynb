{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4967e338-26e7-4be2-a4fe-eefa19cad483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "import s5_postprocess_evaluation as pe\n",
    "import configparser\n",
    "import s6_display_results as dr\n",
    "import s4_evaluation as evl\n",
    "# access parent directory from notebooks directory\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c57e6-348b-4856-98dc-2db1054fc8b7",
   "metadata": {},
   "source": [
    "#### For the reflection example, we are taking the following usecase: mistral medium (as tst and evaluator), non-parallel data prompt, 5 shots and no-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "070f3004-8872-49e4-b822-fb72a5bf36d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileID</th>\n",
       "      <th>user</th>\n",
       "      <th>promptID</th>\n",
       "      <th>model</th>\n",
       "      <th>shots</th>\n",
       "      <th>runID</th>\n",
       "      <th>original</th>\n",
       "      <th>rewritten_sentence</th>\n",
       "      <th>your_text</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "      <th>explanation_content_preservation</th>\n",
       "      <th>explanation_fluency</th>\n",
       "      <th>score_accuracy</th>\n",
       "      <th>score_content_preservation</th>\n",
       "      <th>score_fluency</th>\n",
       "      <th>prompting</th>\n",
       "      <th>evaluator</th>\n",
       "      <th>isParallel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>U1</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>5</td>\n",
       "      <td>911714</td>\n",
       "      <td>I'm all about fast food. I'm having some fries...</td>\n",
       "      <td>Hey, I'm all about that fast food, y'all! I'm ...</td>\n",
       "      <td>I just love fast food! I’ll have some fries, s...</td>\n",
       "      <td>Score: 4\\n\\nExplanation: Both paragraphs S1 an...</td>\n",
       "      <td>Score: 10\\nExplanation: Both S1 and S2 convey ...</td>\n",
       "      <td>Score: 9\\n\\nExplanation: The text is highly co...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NoContext</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>231</td>\n",
       "      <td>U1</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>5</td>\n",
       "      <td>911714</td>\n",
       "      <td>Just having this vegan hotdog from the school ...</td>\n",
       "      <td>Ah, just grabbing this vegan hotdog from the s...</td>\n",
       "      <td>Im having this vegan hotdog at school today. I...</td>\n",
       "      <td>Score: 3\\n\\nExplanation: Both paragraphs conve...</td>\n",
       "      <td>Score: 10\\n\\nExplanation: Both paragraphs disc...</td>\n",
       "      <td>Score: 9\\n\\nExplanation: The text is coherent ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NoContext</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>202</td>\n",
       "      <td>U6</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>5</td>\n",
       "      <td>209327</td>\n",
       "      <td>I went to the \"Groene Burger\" fast food restau...</td>\n",
       "      <td>So, I just popped by the 'Groene Burger' fast ...</td>\n",
       "      <td>I went to the “Groene Burger” fast food joint....</td>\n",
       "      <td>Score: 8\\n\\nExplanation: Paragraph S1 has a mo...</td>\n",
       "      <td>Score: 10\\n\\nExplanation: Both S1 and S2 conve...</td>\n",
       "      <td>Score: 10\\nExplanation: The text is highly coh...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NoContext</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fileID user  promptID           model  shots   runID  \\\n",
       "4       231   U1         2  mistral-medium      5  911714   \n",
       "24      231   U1         2  mistral-medium      5  911714   \n",
       "118     202   U6         2  mistral-medium      5  209327   \n",
       "\n",
       "                                              original  \\\n",
       "4    I'm all about fast food. I'm having some fries...   \n",
       "24   Just having this vegan hotdog from the school ...   \n",
       "118  I went to the \"Groene Burger\" fast food restau...   \n",
       "\n",
       "                                    rewritten_sentence  \\\n",
       "4    Hey, I'm all about that fast food, y'all! I'm ...   \n",
       "24   Ah, just grabbing this vegan hotdog from the s...   \n",
       "118  So, I just popped by the 'Groene Burger' fast ...   \n",
       "\n",
       "                                             your_text  \\\n",
       "4    I just love fast food! I’ll have some fries, s...   \n",
       "24   Im having this vegan hotdog at school today. I...   \n",
       "118  I went to the “Groene Burger” fast food joint....   \n",
       "\n",
       "                                  explanation_accuracy  \\\n",
       "4    Score: 4\\n\\nExplanation: Both paragraphs S1 an...   \n",
       "24   Score: 3\\n\\nExplanation: Both paragraphs conve...   \n",
       "118  Score: 8\\n\\nExplanation: Paragraph S1 has a mo...   \n",
       "\n",
       "                      explanation_content_preservation  \\\n",
       "4    Score: 10\\nExplanation: Both S1 and S2 convey ...   \n",
       "24   Score: 10\\n\\nExplanation: Both paragraphs disc...   \n",
       "118  Score: 10\\n\\nExplanation: Both S1 and S2 conve...   \n",
       "\n",
       "                                   explanation_fluency  score_accuracy  \\\n",
       "4    Score: 9\\n\\nExplanation: The text is highly co...             4.0   \n",
       "24   Score: 9\\n\\nExplanation: The text is coherent ...             3.0   \n",
       "118  Score: 10\\nExplanation: The text is highly coh...             8.0   \n",
       "\n",
       "     score_content_preservation  score_fluency  prompting       evaluator  \\\n",
       "4                          10.0            9.0  NoContext  mistral-medium   \n",
       "24                         10.0            9.0  NoContext  mistral-medium   \n",
       "118                        10.0           10.0  NoContext  mistral-medium   \n",
       "\n",
       "     isParallel  \n",
       "4         False  \n",
       "24        False  \n",
       "118       False  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dr.get_input_data()\n",
    "###  'cutting' the input dataframe, evaluator is mistral medium, non-parallel data only (promptID 2), \n",
    "df = df[(df.evaluator == 'mistral-medium') & (df.isParallel == False) & (df.shots == 5) & (df.prompting == 'NoContext') & (df.model == 'mistral-medium')]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dbb66fb4-6c28-4027-be81-b4c84a29b92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 18)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8ae9d-76c9-4b6a-8741-ddb4881d5378",
   "metadata": {},
   "source": [
    "### Perform the evaluation step (one more time) - only on accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa7c87b4-aeae-4543-ac54-f3e414fb4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict_new_accuracy = {\n",
    "    # Generic prompt\n",
    "    'prompt_llm': 'You are an expert in text style transfer. Here is text T1, supposedly writen in a style of person X: {} ',\n",
    "                 \n",
    "    # Accuracy\n",
    "    'prompt_s2': 'and here is another text T2, containing are a set of unrelated sentences (separated with ;), that are actually written by person X: {} ',\n",
    "    'prompt_inference': 'How different is the conversational style in T1 and T2 on a continuous scale from 1 (completely identical styles) to 10 (completely different styles)? For scoring focus on semantics and syntax. Difference in discussed topics or contexts is irrelevant for the score. Result = . Format result as \"score\" and \"explanation\".',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4cc8daa-a94a-4ed6-9044-6374d53131fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evl_df = pd.DataFrame()\n",
    "for index, row in df.iterrows():\n",
    "    evl_df = pd.concat([evl_df, evl.get_updated_evaluation_mistral(prompts_dict_new_accuracy, row, 'NoContext_' + format(index))], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9330fac9-f0cb-4120-afdf-df8b37625c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "evl_df.to_csv('f8_llm_evaluation_data/Mistral/0_loop_updated_accuracy_evaluation_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fe1e2f06-c1e7-4b92-afe3-8a93db8e2634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rewritten_sentence</th>\n",
       "      <th>score_accuracy</th>\n",
       "      <th>explanation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey, I'm all about that fast food, y'all! I'm ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Score: 8\\n\\nExplanation: The conversational st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ah, just grabbing this vegan hotdog from the s...</td>\n",
       "      <td>6</td>\n",
       "      <td>Score: 6\\n\\nExplanation: The conversational st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So, I just popped by the 'Groene Burger' fast ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Score: 6\\n\\nExplanation: Although both texts a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So, you know that vegan fried chicken from KFC...</td>\n",
       "      <td>6</td>\n",
       "      <td>Score: 6\\n\\nExplanation:\\n\\nAlthough both text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So you know what, I just went to this 'Groene ...</td>\n",
       "      <td>7</td>\n",
       "      <td>Score: 7\\n\\nExplanation: Although both texts a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Hey, just had my first vegan cake at 'groene b...</td>\n",
       "      <td>7</td>\n",
       "      <td>Score: 7\\n\\nExplanation: The conversational st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Sure thing, getting my daily dose of vitamins ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Score: 6\\n\\nExplanation: The syntax and semant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>So, you know what, I'm totally into fast food....</td>\n",
       "      <td>6</td>\n",
       "      <td>Score: 6\\n\\nExplanation:\\n\\nAlthough the texts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>You know what, I'm just vibing with this vegan...</td>\n",
       "      <td>7</td>\n",
       "      <td>Score: 7\\n\\nExplanation: The conversational st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Hmm, just had this vegan hotdog from the schoo...</td>\n",
       "      <td>7</td>\n",
       "      <td>Score: 7\\n\\nExplanation: The conversational st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   rewritten_sentence score_accuracy  \\\n",
       "0   Hey, I'm all about that fast food, y'all! I'm ...              8   \n",
       "1   Ah, just grabbing this vegan hotdog from the s...              6   \n",
       "2   So, I just popped by the 'Groene Burger' fast ...              6   \n",
       "3   So, you know that vegan fried chicken from KFC...              6   \n",
       "4   So you know what, I just went to this 'Groene ...              7   \n",
       "..                                                ...            ...   \n",
       "91  Hey, just had my first vegan cake at 'groene b...              7   \n",
       "92  Sure thing, getting my daily dose of vitamins ...              6   \n",
       "93  So, you know what, I'm totally into fast food....              6   \n",
       "94  You know what, I'm just vibing with this vegan...              7   \n",
       "95  Hmm, just had this vegan hotdog from the schoo...              7   \n",
       "\n",
       "                                 explanation_accuracy  \n",
       "0   Score: 8\\n\\nExplanation: The conversational st...  \n",
       "1   Score: 6\\n\\nExplanation: The conversational st...  \n",
       "2   Score: 6\\n\\nExplanation: Although both texts a...  \n",
       "3   Score: 6\\n\\nExplanation:\\n\\nAlthough both text...  \n",
       "4   Score: 7\\n\\nExplanation: Although both texts a...  \n",
       "..                                                ...  \n",
       "91  Score: 7\\n\\nExplanation: The conversational st...  \n",
       "92  Score: 6\\n\\nExplanation: The syntax and semant...  \n",
       "93  Score: 6\\n\\nExplanation:\\n\\nAlthough the texts...  \n",
       "94  Score: 7\\n\\nExplanation: The conversational st...  \n",
       "95  Score: 7\\n\\nExplanation: The conversational st...  \n",
       "\n",
       "[96 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_step = evl_df[['rewritten_sentence','score_accuracy','explanation_accuracy']]\n",
    "eval_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2775a598-2cf9-443c-96d9-fb02337a1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, value in eval_step['explanation_accuracy'].items():\n",
    "#     # Find the index of \"Explanation:\"\n",
    "#     index = value.index(\"Explanation:\") + len(\"Explanation:\")\n",
    "#     # Extract the text after \"Explanation:\"\n",
    "#     value = value[index:]\n",
    "#     # print(i,value,'\\n')\n",
    "#     eval_step['explanation_accuracy'].iloc[i] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "729e164a-1c2d-4167-98ca-b5c74341e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_score_accuracy'] = eval_step['score_accuracy']\n",
    "df['new_explanation_accuracy'] = eval_step['explanation_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b24ac28c-c562-4b99-80d6-206a81036752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.895833333333333, 7.0)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['score_accuracy'].astype(float).mean(),df['new_score_accuracy'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11192a86-09df-4f80-af51-4f1df79a0c09",
   "metadata": {},
   "source": [
    "### Perform the feedback step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "359cb94e-2bde-43c3-aeb9-e21bd7faaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict_feedback = {\n",
    "    # Generic prompt\n",
    "    'prompt_llm': 'You are an AI model that improves a text style transfer based on provided feedback. Here is text T1, supposedly writen in a style of person X: {} ',\n",
    "                 \n",
    "    # Accuracy\n",
    "    'prompt_s2': 'and here is another text T2, containing are a set of unrelated sentences (separated with ;), that are actually written by person X: {} ',\n",
    "    'prompt_inference': 'Use the following feedback on text style difference between T1 and T2: {}, to rewrite T1 so that it is more similar in style to T2. Keep the style conversational and informal. Result = . Format result as \"sentence\" and \"explanation\".',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e74952ec-da4d-4b7a-b663-e3d3bc7367fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = pd.DataFrame()\n",
    "for index, row in df.iterrows():\n",
    "    feedback_df = pd.concat([feedback_df, evl.get_updated_evaluation_mistral(prompts_dict_feedback, row, 'NoContext_' + format(index),'feedback_generation')], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "577f9698-a333-44d8-aa7e-2b398fc2d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df.to_csv('f8_llm_evaluation_data/Mistral/0_loop_updated_feedback_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0aeb399e-a317-459b-8f81-bbf302044e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotes(s):\n",
    "    if s.startswith(' \"'):\n",
    "        s = s[2:]\n",
    "    if s.endswith('\"'):\n",
    "        s = s[:-1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "46c7e6f8-3b56-42f2-960d-bd0ea4a0909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in feedback_df.iterrows():\n",
    "    s = remove_quotes(r['explanation_feedback'].split(':')[1].split('\\n')[0])\n",
    "    if s:\n",
    "        feedback_df['tst_feedback'].iloc[i] = s \n",
    "    else:\n",
    "        # some sentences are messed up, as Sentence: is followed with new line, so they require different handling \n",
    "        feedback_df['tst_feedback'].iloc[i] = r['explanation_feedback'].split(':')[1].strip().split('\\n')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b9230-52ec-47c2-be4b-d0f431494f7c",
   "metadata": {},
   "source": [
    "### Evaluate the feedback - regenerate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b0108218-39cb-4651-82c6-452b05833675",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict_new_accuracy = {\n",
    "    # Generic prompt\n",
    "    'prompt_llm': 'You are an expert in text style transfer. Here is text T1, supposedly writen in a style of person X: {} ',\n",
    "                 \n",
    "    # Accuracy\n",
    "    'prompt_s2': 'and here is another text T2, containing are a set of unrelated sentences (separated with ;), that are actually written by person X: {} ',\n",
    "    'prompt_inference': 'How different is the conversational style in T1 and T2 on a continuous scale from 1 (completely identical styles) to 10 (completely different styles)? For scoring focus on semantics and syntax. Difference in discussed topics or contexts is irrelevant for the score. Result = . Format result as \"score\" and \"explanation\".',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0c5c1897-2af8-4a43-9117-a2c1cf734b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_tst_evl_df = pd.DataFrame()\n",
    "for index, row in feedback_df.iterrows():\n",
    "    feedback_tst_evl_df = pd.concat([feedback_tst_evl_df, evl.get_updated_evaluation_mistral(prompts_dict_new_accuracy, row, 'NoContext_' + format(index),'feedback_evaluation')], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4d73822d-2b41-4eaa-9a3e-d4ca115f56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_tst_evl_df.to_csv('f8_llm_evaluation_data/Mistral/0_loop_accuracy_feedback_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3180ba04-52a0-4cf3-a14e-8cbf4556e767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'fileID', 'user', 'promptID', 'model', 'shots', 'runID',\n",
       "       'original', 'rewritten_sentence', 'your_text', 'explanation_accuracy',\n",
       "       'explanation_content_preservation', 'explanation_fluency',\n",
       "       'score_accuracy', 'score_content_preservation', 'score_fluency',\n",
       "       'prompting', 'evaluator', 'isParallel', 'new_score_accuracy',\n",
       "       'new_explanation_accuracy', 'tst_feedback', 'explanation_feedback',\n",
       "       'accuracy_feedback', 'explanation_feedback_evaluation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback_tst_evl_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c6f035d9-bdc3-402c-9899-defc49090ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Score: 7\\n\\nExplanation: While both texts are written in an informal style, there are notable differences in semantics and syntax that contribute to a score of 7 on a 10-point scale.\\n\\nIn T1, the text is descriptive and focuses on the speaker\\'s current actions and plans, using present continuous tense (\"I\\'m munching\", \"sipping\", \"looking forward\") and concrete nouns (\"fries\", \"soy burger\", \"coke\", \"football game\"). The sentence structure is simple and straightforward.\\n\\nIn contrast, T2 consists of several unrelated sentences that are more interactive and dialogue-based, with the speaker responding to different prompts or addressing different people (\"tell me about your family\", \"I\\'ll do the export\", \"im curious\", \"no skiing for me\"). The sentences are shorter and more fragmented, with the use of ellipses and exclamation marks to convey the speaker\\'s tone. There is also more variation in sentence structure, with questions, statements, and commands.\\n\\nOverall, while both texts reflect the informal and conversational style of person X, the differences in semantics and syntax make them distinct enough to warrant a score of 7 on a 10-point scale.Score: 6\\n\\nExplanation: \\nThe conversational style in T1 and T2 differ in terms of semantics and syntax. In T1, person X is describing their experience with a vegan hotdog, using descriptive words such as \"nice crunch\" and \"bit of a kick\". The sentence structure is simple and straightforward, with a focus on the action of trying the hotdog.\\n\\nOn the other hand, T2 contains a set of unrelated sentences that vary in syntax and semantics. For example, the first sentence \"ah I think its fine! this is probably related to their privacy statement\" has a more informal tone and uses an exclamation mark for emphasis. The third sentence \"i\\'ll do the export, so yeah we are done!\" uses contractions and has a more task-oriented tone. The fifth sentence \"no skiing for me, I never learned it. I think im a bit scared to do that\" uses first-person pronouns and expresses a personal opinion.\\n\\nWhile the topics discussed in T1 and T2 are different, the differences in semantics and syntax contribute to a score of 6 out of 10 on the continuous scale. The styles are not completely different, as person X still uses informal language and contractions in both texts. However, the variation in sentence structure, tone, and purpose make them distinct enough to warrant a moderate score.'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(feedback_tst_evl_df['explanation_feedback_evaluation'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d37ca34c-8eca-434c-bf61-bcd463016fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.895833333333333, 7.0, 5.6875)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['score_accuracy'].astype(float).mean(),df['new_score_accuracy'].astype(float).mean(),feedback_tst_evl_df['accuracy_feedback'].astype(float).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e25b67-18e5-4e39-afa5-98892517455d",
   "metadata": {},
   "source": [
    "# Second loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb0fc1-66e6-4afa-891a-7fa4b0215dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = pd.DataFrame()\n",
    "for index, row in feedback_tst_evl_df.iterrows():\n",
    "    feedback_df = pd.concat([feedback_df, evl.get_updated_evaluation_mistral(prompts_dict_feedback, row, 'NoContext_' + format(index),'feedback_generation')], ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-evaluate)",
   "language": "python",
   "name": "llm-evaluate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
