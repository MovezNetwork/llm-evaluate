{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "# access parent directory from notebooks directory\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T11:05:04.866578Z",
     "start_time": "2024-02-22T11:05:03.599588Z"
    }
   },
   "id": "bcc635030aa20c5",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation Mistral"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d01f6c5a12249e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682\n",
      "output_evaluation/Mistral/NoContext/evaluation_NoContext_1153_mistral-medium.csv\n",
      "score not found list index out of range\n"
     ]
    }
   ],
   "source": [
    "output_evaluation_folder_path = 'output_evaluation/'\n",
    "output_evaluation_mistral_folder_name = output_evaluation_folder_path + 'Mistral/'\n",
    "\n",
    "prompting_strategies = ['NoContext', 'Context']\n",
    "\n",
    "for prompting in prompting_strategies:\n",
    "    output_df = pd.DataFrame()\n",
    "    for subdir, dirs, files in os.walk(output_evaluation_mistral_folder_name+prompting):\n",
    "        file_id = 0\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if filepath.endswith(\"mistral-medium.csv\"):\n",
    "                temp_output = pd.read_csv(filepath)\n",
    "                accuracy = format(temp_output[['explanation_accuracy']].iloc[0])\n",
    "                temp_output['score_accuracy'] = accuracy.split('\\\\')[0].split(': ')[1]\n",
    "                try:\n",
    "                    content_preservation = format(temp_output[['explanation_content_preservation']].iloc[0])\n",
    "                    temp_output['score_content_preservation'] = content_preservation.split('\\\\')[0].split(': ')[1]\n",
    "                except Exception as e:\n",
    "                    print(file_id)\n",
    "                    print(filepath)\n",
    "                    print(\"score not found\", e)\n",
    "                    temp_output['score_content_preservation'] = \"\"\n",
    "                    \n",
    "                fluency = format(temp_output[['explanation_fluency']].iloc[0])\n",
    "                temp_output['score_fluency'] = fluency.split('\\\\')[0].split(': ')[1]\n",
    "                temp_output['prompting'] = prompting \n",
    "                \n",
    "                # concat temp dataframe\n",
    "                output_df = pd.concat([output_df, temp_output], ignore_index=True)\n",
    "                file_id = file_id + 1\n",
    "\n",
    "    # save to csv\n",
    "    output_df.to_csv(output_evaluation_folder_path + 'Evaluation_' + prompting + '_mistral-medium.csv', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:30:24.251237Z",
     "start_time": "2024-02-22T12:29:53.968983Z"
    }
   },
   "id": "78ac1000d1da01cf",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation using GPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6de2a72b845686"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output_evaluation_folder_path = 'output_evaluation/'\n",
    "output_evaluation_mistral_folder_name = output_evaluation_folder_path + 'GPT/'\n",
    "\n",
    "prompting_strategies = ['NoContext', 'Context']\n",
    "\n",
    "for prompting in prompting_strategies:\n",
    "    output_df = pd.DataFrame()\n",
    "    for subdir, dirs, files in os.walk(output_evaluation_mistral_folder_name+prompting):\n",
    "        file_id = 0\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if filepath.endswith(\"gpt-4.csv\"):\n",
    "                temp_output = pd.read_csv(filepath)\n",
    "                accuracy = format(temp_output[['explanation_accuracy']].iloc[0])\n",
    "                temp_output['score_accuracy'] = accuracy.split('\\\\')[0].split(': ')[1]\n",
    "                try:\n",
    "                    content_preservation = format(temp_output[['explanation_content_preservation']].iloc[0])\n",
    "                    temp_output['score_content_preservation'] = content_preservation.split('\\\\')[0].split(': ')[1]\n",
    "                except Exception as e:\n",
    "                    print(file_id)\n",
    "                    print(filepath)\n",
    "                    print(\"score not found\", e)\n",
    "                    temp_output['score_content_preservation'] = \"\"\n",
    "\n",
    "                fluency = format(temp_output[['explanation_fluency']].iloc[0])\n",
    "                temp_output['score_fluency'] = fluency.split('\\\\')[0].split(': ')[1]\n",
    "                temp_output['prompting'] = prompting\n",
    "\n",
    "                # concat temp dataframe\n",
    "                output_df = pd.concat([output_df, temp_output], ignore_index=True)\n",
    "                file_id = file_id + 1\n",
    "\n",
    "    # save to csv\n",
    "    output_df.to_csv(output_evaluation_folder_path + 'Evaluation_' + prompting + '_gpt-4.csv', index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:31:45.710140Z",
     "start_time": "2024-02-22T12:31:11.582766Z"
    }
   },
   "id": "cf3432bb75ba2763",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summarize scores"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac51515be9c5db52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# manually checked the files for postprocessing. In a few occasions the score was not correctly extracted from the explanation. In these cases the scores were adjusted manually\n",
    "\n",
    "mistral_noContext = pd.read_csv(output_evaluation_folder_path + 'Evaluation_NoContext_mistral-medium_corrected.csv')\n",
    "mistral_context = pd.read_csv(output_evaluation_folder_path + 'Evaluation_Context_gpt-4_corrected.csv')\n",
    "gpt_noContext = pd.read_csv(output_evaluation_folder_path + 'Evaluation_NoContext_gpt-4_corrected.csv')\n",
    "gpt_context = pd.read_csv(output_evaluation_folder_path + 'Evaluation_Context_gpt-4_corrected.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:05:33.444015Z",
     "start_time": "2024-02-22T13:05:33.042424Z"
    }
   },
   "id": "4e2a887833f804bc",
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summ_mistral_noContext = mistral_noContext.groupby(['model', 'promptID',  'shots']).agg({'score_accuracy': 'mean', 'score_content_preservation': 'mean', 'score_fluency': 'mean'})\n",
    "summ_mistral_noContext.to_csv(output_evaluation_folder_path + 'Evaluation_NoContext_mistral-medium_summary.csv', index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:09:06.550289Z",
     "start_time": "2024-02-22T13:09:06.516693Z"
    }
   },
   "id": "64b62ad5cfdf9bcf",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summ_mistral_context = mistral_context.groupby(['model', 'promptID', 'shots']).agg({'score_accuracy': 'mean', 'score_content_preservation': 'mean', 'score_fluency': 'mean'})\n",
    "summ_mistral_context.to_csv(output_evaluation_folder_path + 'Evaluation_Context_mistral-medium_summary.csv', index=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:13:16.647851Z",
     "start_time": "2024-02-22T13:13:16.613325Z"
    }
   },
   "id": "cf3122a13a773123",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summ_gpt_noContext = mistral_noContext.groupby(['model', 'promptID', 'shots']).agg({'score_accuracy': 'mean', 'score_content_preservation': 'mean', 'score_fluency': 'mean'})\n",
    "summ_gpt_noContext.to_csv(output_evaluation_folder_path + 'Evaluation_NoContext_gpt-4_summary.csv', index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:13:17.146217Z",
     "start_time": "2024-02-22T13:13:17.110045Z"
    }
   },
   "id": "2bfd0dd136e6c11e",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "summ_gpt_context = mistral_context.groupby(['model', 'promptID', 'shots']).agg({'score_accuracy': 'mean', 'score_content_preservation': 'mean', 'score_fluency': 'mean'})\n",
    "summ_gpt_context.to_csv(output_evaluation_folder_path + 'Evaluation_Context_gpt-4_summary.csv', index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:13:17.547902Z",
     "start_time": "2024-02-22T13:13:17.508062Z"
    }
   },
   "id": "52dce6d10c139838",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec69ff5e406d3b02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
